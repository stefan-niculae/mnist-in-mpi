\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}

\usepackage{amsmath}  % equations
\usepackage{cite}  % bibliograby
\usepackage{graphicx}  % images
\usepackage{pgfplots}  % plots
  \pgfplotsset{width=10cm,compat=1.9} % plots settings
  \usepgfplotslibrary{external}
  \tikzexternalize
\usepackage{todonotes}
\usepackage{wrapfig}



\usepackage{hyperref}  % references
  \hypersetup{colorlinks=true,citecolor=black,filecolor=black,linkcolor=black,urlcolor=black} % references settings

% \usepackage[automark]{scrpage2} % heading
%   \pagestyle{scrheadings}
%   \ihead[]{Handwritten character recognition in MPI} % left header
%   \ohead[]{\today} % right header
%   \cfoot[]{\pagemark} % footer
%   \setheadsepline[122mm]{0.3mm}


% floor operator
\newcommand{\floor}[1]{\lfloor {#1} \rfloor}
\newcommand{\TODO}[1]{\noindent \textbf{\textcolor{red}{TODO: #1}}}


\begin{document}
	\title{
	\Huge Handwritten digit recognition
	}
	
	\vspace{2cm}
	
	\author{\Large \href{mailto:stefan.niculae@my.fmi.unibuc.ro}{Stefan Niculae} \and \Large \href{mailto:ionut.ciocoiu@my.fmi.unibuc.ro}{Ionut Ciocoiu}
	\vspace{3cm}}
	
	\date{
	\large Parallel \& Concurrent Programming Lab Report \\
    \vspace{0.2cm}
	\today
	}

	\maketitle
% 	\setlength{\parindent}{0pt}

\vspace{5cm}
\begin{abstract}
We implement a neural network to classify handwritten digits with training done in parallel, using MPI. We present the dataset, model architecture, parallelization strategy and an interactive interface. Then we discuss model accuracy evolution and time performance benchmarks. Our implementation achieves good prediction performance with fast training time.
\end{abstract}



% \newpage
% \pagestyle{plain}
% \tableofcontents



\newpage
\section{Introduction}
The project aims to highlight the role of parallelism in programming, especially in Machine Learning tasks. Considering the vast amount of data today's applications have to deal with, it is paramount to run tasks concomitantly. To underline  improvements bought by parallelism, we implemented  a neural network training module in MPI.

In the following sections we will talk about the dataset used; detail model architecture and reason for choosing it along with its training performance. Parallelism has a dedicated section with a high-level overview of the solution and time performance benchmarks. The last sections contain an outline of the graphical interface and implementation details for the entire project.

 
\section{Problem statement}
\label{sec:problem}
Given a picture of a handwritten digit, label it accordingly 0 - 9.
This is a classic problem in Computer Vision on which we apply a neural network trained using gradient descent. We seek to reach satisfactory model performance and fast training time, using parallelism.


\section{Dataset}
\label{sec:dataset}
We demonstrate our model performance on the MNIST database, widely used dataset for training and testing in the field of machine learning. It contains 70 000 examples of labeled handwritten digits. The digits have been size-normalized and centered in a fixed-size image \cite{mnist}. 

\begin{figure}[htb]
\centering
\includegraphics[width=8cm]{images/mnist-images.png}
\caption{Sample MNIST images \cite{tf}}
\end{figure}


The images are black-and-white, with a resolution of $28 \times 28$ pixels, represented as an array of $784$ values. Each value ranges from $0$ to $1$ indicating the amount of blackness in the pixel.

\begin{figure}[htb]
\centering
\includegraphics[width=8cm]{images/mnist-matrix.png}
\caption{Matrix representation of a digit \cite{tf}}
\end{figure}


\newpage
\section{Model}
\label{sec:model}
In order to recognize a given image, we need to model to understand the data. It needs to be able to be queried for a new instance in order to provide an interactive demo.
\\

We use a feed-forward neural network with an one fully connected layer:
\begin{align*}
y &= f(Wx + b) \\
label &= argmax(y)
\end{align*}

Where $W$ is the weights matrix, $10 \times 784$ -- the number of classes(one for each digit) and the dimension of an image ($784$). $b$ is the bias vector, $1 \times 10$, corresponding to the free element. $y$ is a vector of probabilities, from which argmax extracts the predicted labe
\\

The activation function output a vector of probabilities:
$$f(x_i)=\frac{exp(x_i)}{\sum_j exp(x_j)}$$
Where $exp(x)$ represents $e^x$, Euler's constant. It \textit{normalizes} the vector, causing the greatest elements to have logarithmic relative value.
\\

\begin{figure}[htb]
\centering
\includegraphics[width=8cm]{images/model-graph.png}
\caption{Simplified representation of model graph \cite{tf}}
\end{figure}

The cost function is cross-entropy:
$$H(y_t, y_p)=-\sum_x y_t log(y_p)$$
Where $y_t$ is the true, correct label of an image and $y_p$ is the label predicted by the model.
\\

$L_2$ regularization is used:
$$\frac{1}{\lambda} \sum_i \sum_j W_{i,j}^2$$
Where $\lambda$ is the regularization parameter. This limits over-fitting by imposing a penalty on big weights.
\\

The model uses an adaptive learning rate:
$$\eta \frac{ W^{(k)} - W^{(k-1)} } { \nabla^{(k)} - \nabla^{(k-1)} }$$
Where $\eta$ is the momentum parameter. This helps with stability in late epochs.

\paragraph{Training} We employ gradient-descent on mini-batches. 
\begin{align*}
W^{(k)} &= W^{(k-1)} + \alpha \nabla_W^{(k-1)} \\ 
b^{(k)} &= b^{(k-1)} + \alpha \nabla_b^{(k-1)}
\end{align*}
Where $W$ is the weights matrix, $10 \times 784$ -- the number of classes( one for each digit) and the dimension of an image ($28 \times 28$). $b$ is the bias vector, $1 \times 10$, corresponding to the free element. $W^{(k)}$ denotes the weights at step $k$. $\nabla$ represents the update which must produce to improve prediction.
\\
Early-stopping prevents the model from further learning when performance on the validation split starts decreasing. Random sampling averts cycles when iterating through the dataset.

\paragraph{Evaluation}
Model performance is measured on a separate testing set using the accuracy metric -- ie: the percentage of images correctly classified.

\noindent The following chart illustrates model evolution, after each pass through the dataset, on the entire MNIST training (60 000) and test (10 000) dataset.

\begin{center}
\begin{tikzpicture}
\begin{axis} [
  title={Training performance},
  xlabel={Epoch},
  ylabel={Accuracy (\%)},
  xmin=0, xmax=250,
  ymin=0, ymax=100,
  enlarge x limits=0.05,
  xtick={0,50,100,150,200, 250},
  ytick={0,25,50,75,100},
  legend pos=south east,
  axis line style={black!65, line width=.1pt}, % thinner frame border
  legend style={draw=black!50, line width=.1pt}, % thinner frame border
  every x tick/.style={white}, % hide x ticks (make them white)
  every y tick/.style={white}, % hide y ticks (make them white)
  every axis plot/.append style={thick},
]

\addplot[dash pattern=on 6pt off 4pt]
  coordinates {
 (0, 25.542341)    (1, 47.745369)    (2, 66.106474)    (3, 79.268610)    (4, 81.409645)    (5, 82.697285)    (6, 83.599091)    (7, 84.318434)    (8, 84.974273)    (9, 85.386156)    (10, 85.663764)    (11, 86.105388)    (12, 86.463159)    (13, 86.705436)    (14, 86.973765)    (15, 87.296094)    (16, 87.391127)    (17, 87.672536)    (18, 87.834651)    (19, 87.989275)    (20, 88.216571)    (21, 88.300424)    (22, 88.423408)    (23, 88.635834)    (24, 88.689947)    (25, 88.783079)    (26, 88.917243)    (27, 89.086849)    (28, 89.001096)    (29, 89.200441)    (30, 89.248964)    (31, 89.334605)    (32, 89.433439)    (33, 89.565703)    (34, 89.670015)    (35, 89.733408)    (36, 89.835932)    (37, 89.940244)    (38, 89.940244)    (39, 90.016606)    (40, 90.037178)    (41, 90.124720)    (42, 90.079998)    (43, 90.175031)    (44, 90.219753)    (45, 90.365097)    (46, 90.448950)    (47, 90.372588)    (48, 90.560753)    (49, 90.534703)    (50, 90.560753)    (51, 90.687426)    (52, 90.603574)    (53, 90.763900)    (54, 90.821590)    (55, 90.810410)    (56, 90.843951)    (57, 90.920425)    (58, 90.901753)    (59, 90.914834)    (60, 90.948375)    (61, 91.037818)    (62, 91.007967)    (63, 91.136541)    (64, 91.125360)    (65, 91.220393)    (66, 91.147721)    (67, 91.233474)    (68, 91.328507)    (69, 91.334097)    (70, 91.384409)    (71, 91.255835)    (72, 91.330296)    (73, 91.438410)    (74, 91.348967)    (75, 91.430919)    (76, 91.522262)    (77, 91.542722)    (78, 91.546524)    (79, 91.531542)    (80, 91.596835)    (81, 91.555803)    (82, 91.624786)    (83, 91.643345)    (84, 91.783100)    (85, 91.768230)    (86, 91.712328)    (87, 91.727198)    (88, 91.818541)    (89, 91.840902)    (90, 91.755149)    (91, 91.900493)    (92, 91.846492)    (93, 91.870754)    (94, 91.909885)    (95, 91.971377)    (96, 91.909885)    (97, 92.034657)    (98, 91.904295)    (99, 91.973165)    (100, 91.982557)    (101, 92.010508)    (102, 92.006706)    (103, 92.012297)    (104, 92.083180)    (105, 92.045838)    (106, 92.131591)    (107, 92.213543)    (108, 92.072000)    (109, 92.189393)    (110, 92.146461)    (111, 92.254575)    (112, 92.245295)    (113, 92.185592)    (114, 92.293706)    (115, 92.273246)    (116, 92.258264)    (117, 92.381248)    (118, 92.278836)    (119, 92.474492)    (120, 92.358887)    (121, 92.375658)    (122, 92.476281)    (123, 92.448330)    (124, 92.351508)    (125, 92.511722)    (126, 92.425969)    (127, 92.388739)    (128, 92.360788)    (129, 92.450231)    (130, 92.502443)    (131, 92.565723)    (132, 92.543363)    (133, 92.621625)    (134, 92.647787)    (135, 92.532182)    (136, 92.560133)    (137, 92.614246)    (138, 92.629116)    (139, 92.578804)    (140, 92.696198)    (141, 92.703689)    (142, 92.586295)    (143, 92.714869)    (144, 92.727838)    (145, 92.657067)    (146, 92.737230)    (147, 92.714869)    (148, 92.660756)    (149, 92.711068)    (150, 92.726050)    (151, 92.735329)    (152, 92.843443)    (153, 92.789330)    (154, 92.828461)    (155, 92.742820)    (156, 92.766970)    (157, 92.876984)    (158, 92.875083)    (159, 92.722248)    (160, 92.794920)    (161, 92.871394)    (162, 92.826673)    (163, 92.878773)    (164, 92.938476)    (165, 92.882574)    (166, 92.958936)    (167, 92.882574)    (168, 92.942165)    (169, 92.895543)    (170, 92.785641)    (171, 92.988788)    (172, 92.953346)    (173, 92.899345)    (174, 92.951445)    (175, 93.014838)    (176, 93.026018)    (177, 92.981297)    (178, 93.018527)    (179, 92.975707)    (180, 93.016738)    (181, 93.024117)    (182, 93.024117)    (183, 93.009248)    (184, 92.984986)    (185, 93.085609)    (186, 92.975707)    (187, 93.052068)    (188, 93.095001)    (189, 93.027919)    (190, 93.083820)    (191, 93.087510)    (192, 92.975707)    (193, 93.063249)    (194, 93.020428)    (195, 92.930985)    (196, 93.135921)    (197, 93.100591)    (198, 93.111771)    (199, 93.115461)    (200, 93.130331)    (201, 93.206804)    (202, 93.121051)    (203, 93.122952)    (204, 93.063249)    (205, 93.171362)    (206, 93.271986)    (207, 93.076330)    (208, 93.156493)    (209, 93.204903)    (210, 93.225363)    (211, 93.197413)    (212, 93.162083)    (213, 93.221674)    (214, 93.242134)    (215, 93.141511)    (216, 93.227264)    (217, 93.201214)    (218, 93.247724)    (219, 93.271986)    (220, 93.234755)    (221, 93.245935)    (222, 93.208593)    (223, 93.307427)    (224, 93.262706)    (225, 93.271986)    (226, 93.298036)    (227, 93.275675)    (228, 93.260805)    (229, 93.130331)    (230, 93.184443)    (231, 93.251525)    (232, 93.214183)    (233, 93.296247)    (234, 93.283166)    (235, 93.296247)    (236, 93.251525)    (237, 93.318608)    (238, 93.324198)    (239, 93.314806)    (240, 93.385690)    (241, 93.305527)    (242, 93.333477)    (243, 93.335378)    (244, 93.255215)    (245, 93.337167)    (246, 93.329788)    (247, 93.286855)    (248, 93.372609)    (249, 93.331577)    (250, 93.314806)
  };
\addplot[]
  coordinates {
(0, 24.432432)  (1, 44.549688)(2, 56.596855)(3, 62.102859)(4, 66.083265)(5, 68.563395)(6, 71.450773)(7, 70.905093)(8, 73.653296)(9, 74.766428)(10, 75.325960)(11, 76.262903)(12, 75.887467)(13, 77.394473)(14, 78.915509)(15, 78.533935)(16, 77.963218)(17, 78.724613)(18, 79.106624)(19, 79.873274)(20, 79.106624)(21, 79.297958)(22, 80.065484)(23, 80.643426)(24, 80.643426)(25, 79.681283)(26, 79.681283)(27, 79.489511)(28, 80.450560)(29, 81.029814)(30, 80.836510)(31, 81.611037)(32, 81.805215)(33, 81.417077)(34, 81.611037)(35, 81.611037)(36, 81.417077)(37, 82.779383)(38, 82.194226)(39, 82.194226)(40, 81.999611)(41, 82.584112)(42, 83.366506)(43, 82.779383)(44, 82.584112)(45, 83.955593)(46, 83.759013)(47, 83.759013)(48, 83.955593)(49, 84.744100)(50, 84.744100)(51, 83.759013)(52, 84.546646)(53, 83.759013)(54, 84.546646)(55, 84.941772)(56, 83.955593)(57, 83.759013)(58, 82.974872)(59, 84.152393)(60, 84.349410)(61, 84.744100)(62, 84.744100)(63, 85.139662)(64, 85.139662)(65, 85.734641)(66, 84.546646)(67, 85.933403)(68, 85.536096)(69, 85.139662)(70, 86.132384)(71, 85.734641)(72, 85.734641)(73, 86.730633)(74, 85.734641)(75, 85.139662)(76, 85.536096)(77, 85.734641)(78, 85.933403)(79, 86.331582)(80, 85.734641)(81, 85.139662)(82, 86.730633)(83, 85.139662)(84, 85.139662)(85, 86.930485)(86, 86.530999)(87, 86.530999)(88, 85.139662)(89, 86.132384)(90, 86.530999)(91, 86.530999)(92, 86.132384)(93, 86.530999)(94, 86.331582)(95, 86.530999)(96, 86.730633)(97, 86.132384)(98, 86.530999)(99, 87.130555)(100, 86.930485)(101, 86.331582)(102, 86.730633)(103, 86.530999)(104, 86.530999)(105, 85.734641)(106, 86.730633)(107, 86.530999)(108, 86.730633)(109, 85.337770)(110, 86.331582)(111, 85.734641)(112, 86.132384)(113, 86.530999)(114, 86.331582)(115, 86.331582)(116, 86.132384)(117, 86.331582)(118, 86.530999)(119, 86.930485)(120, 86.930485)(121, 85.933403)(122, 86.331582)(123, 86.930485)(124, 86.930485)(125, 86.530999)(126, 87.330843)(127, 87.130555)(128, 85.933403)(129, 86.132384)(130, 86.331582)(131, 85.933403)(132, 87.130555)(133, 86.530999)(134, 86.730633)(135, 86.331582)(136, 85.734641)(137, 86.730633)(138, 86.930485)(139, 86.930485)(140, 86.530999)(141, 87.531349)(142, 87.330843)(143, 86.331582)(144, 87.130555)(145, 86.530999)(146, 86.930485)(147, 87.531349)(148, 87.130555)(149, 86.530999)(150, 86.930485)(151, 87.933014)(152, 87.330843)(153, 87.330843)(154, 86.930485)(155, 86.930485)(156, 87.330843)(157, 87.130555)(158, 87.732072)(159, 87.330843)(160, 86.730633)(161, 87.531349)(162, 87.732072)(163, 86.930485)(164, 87.130555)(165, 87.933014)(166, 87.531349)(167, 87.330843)(168, 87.130555)(169, 87.933014)(170, 87.933014)(171, 87.330843)(172, 87.732072)(173, 87.531349)(174, 87.531349)(175, 87.531349)(176, 87.732072)(177, 87.732072)(178, 87.732072)(179, 87.732072)(180, 88.134173)(181, 87.531349)(182, 87.732072)(183, 87.933014)(184, 87.531349)(185, 87.531349)(186, 86.530999)(187, 88.537144)(188, 87.330843)(189, 86.930485)(190, 88.335550)(191, 87.330843)(192, 87.933014)(193, 87.130555)(194, 87.130555)(195, 87.330843)(196, 87.732072)(197, 88.134173)(198, 87.330843)(199, 88.134173)(200, 87.933014)(201, 87.130555)(202, 87.531349)(203, 86.930485)(204, 87.130555)(205, 87.330843)(206, 87.732072)(207, 87.933014)(208, 87.130555)(209, 87.933014)(210, 87.330843)(211, 87.933014)(212, 87.933014)(213, 87.732072)(214, 88.335550)(215, 87.531349)(216, 87.732072)(217, 87.732072)(218, 87.933014)(219, 88.134173)(220, 86.930485)(221, 87.933014)(222, 87.732072)(223, 87.732072)(224, 88.134173)(225, 87.933014)(226, 88.335550)(227, 88.134173)(228, 87.933014)(229, 88.537144)(230, 87.933014)(231, 87.531349)(232, 87.933014)(233, 87.933014)(234, 87.330843)(235, 87.330843)(236, 87.531349)(237, 87.933014)(238, 86.730633)(239, 87.933014)(240, 87.330843)(241, 87.933014)(242, 88.134173)(243, 87.933014)(244, 88.537144)(245, 87.933014)(246, 87.732072)(247, 87.933014)(248, 87.531349)(249, 88.537144)(250, 88.335550)
  };
  
\legend{train, test}
\end{axis}
\end{tikzpicture}
\end{center}

The training performance is intentionally dashed, as  what interests us is the model's generalization power. It quickly learns in the first 50 epochs and reaches a plateau around the 100 mark. It achieves 93.3\% performance on the training set and 88.3\% on unseen data.



\newpage
\section{Parallelization}
The phase that needs parallelization is the training of the model. Predicting is not computationally intensive. The bulk of the time is spent iterating over the entire training set multiple times. In one epoch, the network looks at every image, in batches and does the following:
\begin{itemize}
\item compute the change that must happen in order to better recognize -- called gradient,
\item updates its internal weights based on the gradient.
\end{itemize}
Computing the gradient takes the most time. The expensive operations are two dot-products on the weights matrices and the a matrix-wise softmax. This is the portion that we will parallelize.

When looking at a batch of images:
\begin{itemize}
\item the batch is assigned evenly among workers,
\item each worker computes the gradient on its chunk of the batch,
\item partial gradients are sent from each worker to the master,
\item the master process accumulates all the partial gradients and aggregates them,
\item the master updates the model weights based on the total gradient,
\item updated weights are sent back to every worker,
\item a new batch is ready to be processed: each worker now knows the effects of all the others.
\end{itemize}
In our case, aggregating the partial gradients requires nothing more then a summation.
\\

Naturally, there appears a running time -- model accuracy trade-off. Updating the weights after every image yields better accuracy than updating once every 1,000 samples. In the case of a greater batch size, the model has not evolved after looking at the first image, nor after the second or the third. It still analyzes them knowing just as much as it did when looking at the first sample.

In contrast, a greater batch size produces better parallelization performance. If each worker is assigned a single image, the communication cost of transferring gradient and weight matrices far outweighs the time taken to compute them. Efficiency improvement is present only when the time taken to transfer the results is lower than the time taken to compute them. Greater batch sizes incur more infrequent updates thus reducing the communication overhead.

The trade-off is now clear: a lower batch size leads to greater accuracy while a greater batch size causes faster training.
\\

Another approach would be to let the algorithm run sequentially and parallelize only the matrix multiplication operation. In light of the previously presented considerations, we can conclude that this would be grossly inefficient. The transfer overhead is much higher than the benefit gained by splitting the problem.

The following chart illustrates the improvement brought by parallelization on one pass through the data:

\begin{center}
\begin{tikzpicture}
\begin{axis}[
  title={Time per epoch},
  xlabel=Processes,
  ylabel=Seconds,
  bar width=1cm,
  enlarge x limits=0.2,
%   enlarge y limits=0.18,
  axis line style={black!65, line width=.1pt}, % thinner border
  symbolic x coords={1,2,4,8,16}, % equidistant bars
  every x tick/.style={white}, % hide x ticks (make them white)
  every y tick/.style={line width=.1pt, draw=gray!10}, % y ticks same color as grid
  ymajorgrids=true, % show horizontal grid lines
  grid style={line width=.1pt, draw=gray!20}, % gray grid lines
  ]
  \addplot[ybar, black!40, fill=black!20] coordinates {
      (1,5.27) 
      (2,2.81)
      (4,1.51)
      (8,0.78)
      (16,0.79)
      };
\end{axis}
\end{tikzpicture}
\end{center}

\noindent The benchmarks were done on the MNIST training set of 60 000 images with a batch size of 1000, on a machine with 20 processors @ 2.60GHz.

The training time is almost halved when going from one (5.27s) to two (2.81s) processors: 88\% improvement. Further 86\% improvement is achieved on four processors (1.51s), less than a third (29\%) of the single-process time. Doubling the number of processes to eight yields a further 94\% improvement (0.78s), making it only 15\% of the single-process time. This is notable when considering the perfect improvement possible of 12.5\%. 

For more than eight cores, the communication overhead starts to take its toll. No further performance improvement is achieved. Chunk sizes get increasingly small (1000, 500, 250, 125, 63), hitting an inflection point at the eight-processes mark.


\newpage
\section{Graphical Interface}


The user is presented with a blank box in which to draw a digit to be recognized. Upon request, the prediction along with the model's confidence in it is displayed. 

\begin{wrapfigure}{r}{0.6\textwidth}\centering
    \includegraphics[width=6cm]{images/gui.png}
\end{wrapfigure}

\noindent Since the model was trained on $28 \times 28$ images, we need to pass the input in the same format. On today's screens, an input box of $28 \times 28$ pixels would look comically small. The raw input must be resized accordingly.

The simplest down-sizing method is taking each block of pixels in the original image and averaging it to form a single pixel in the resulting image. For simplicity, we assume square images and pixels with only one channel -- grayscale. Eg: an image with $n^2$ original pixels is condensed to $s^2$ pixels, meaning a factor of $f=\frac{n}{s}$. That means each of the $s^2$ pixels is a sum of a $f \times f$ block divided by $f^2$.

The trouble appears with encoding the image -- both the raw image and the model's inputs are in array form instead of a matrix. Ie: the pixels are all placed one after another starting from the top left corner.

\begin{center}
\includegraphics[width=\textwidth]{images/resize.png}
\end{center}

The transformation can be computed by \textit{compacting} the columns:
$\sum_i Orig_i$ for column at index $\floor{\frac{i}{f}}$. Then for rows: $\sum_i Col_i$ for pixel at index $(i \bmod s) + \floor{\frac{i}{s \times f}}$. Where $Orig$ denotes the the original image $Col$ the column-wise averages. 







\newpage
\section{Implementation}
After going through the high-level overview of the project, we now delve into implementation details.

\paragraph{Predictive model} The core of the project is implemented in C++. This includes the neural network training and prediction. Model weights and image samples are kept in memory using 2D-arrays. 

Matrices build as vector of vectors (using the standard library implementation), while providing greater flexibility, cannot be sent efficiently using MPI. MPI deals with contiguous blocks of memory, while STL vectors are, by definition, dynamic. A work-around for this is to send a matrix row-by-row. Our initial implementation used this method but unfortunately it provides little to no increase in parallelization performance and caused a general-rewrite. 

The most expensive matrix operations are dot products and exponentiation. For matrix multiplication, we explored the Strassen algorithm, $O(n^{2.8})$, and the Coppersmithâ€“Winograd algorithm, $O(n^{2.37})$. Although they achieve high asymptotic performance, hidden constants make them impractical in our case. We went with the Naive, $\Theta(n^3)$, algorithm with the $ikj$ optimization. For exponentiation, we rely on STL's implementation, although this can be further optimized.

\paragraph{Graphical Interface} The GUI is implemented as a web page in HTML, with a canvas for the input box.
The drawing is facilitated by the Sketch library \cite{sketch}. Interactivity and picture resizing is done through JavaScript.

\paragraph{Communication Server} The glue between the front-end and the model is a web-server done in Python, using the Flask library \cite{flask}. An array of $784$ pixels is received through from the GUI and is passed to the compiled executable. The output (predicted digit and prediction confidence) is then passed back through AJAX.
\\

\noindent The entire source code is available at \url{https://github.com/stefan1niculae/mnist-in-mpi}.

\vspace{\fill} % place references at the bottom
\bibliographystyle{ieeetr}
	\bibliography{references} % expects file "references.bib"
	\addcontentsline{toc}{section}{References}
    
    

\end{document}
